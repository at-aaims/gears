{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elegant-night",
   "metadata": {},
   "source": [
    "# On demand interactive Dask based large scale data analysis on Andes\n",
    "\n",
    "This is an ondemand interactive Dask usage on Andes using Slurm.\n",
    "The cell below documents the underlying behavior of the Dask cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-rugby",
   "metadata": {},
   "source": [
    "## Acquiring Dask cluster\n",
    "\n",
    "The proposed way to spawn for gears a single Dask cluster per notebook.  Each notebook will have an corresponding Dask scheduler anchored on the node which the notebook is running.\n",
    "Here, we want to be sure the scheduler won't step on others and would generate random ports for both the scheduler and the dashboard.   In the case of gears, the notebook can run on a login node where others could be in your way.  Even yourself.\n",
    "\n",
    "The cell below demonstrates how to do this.\n",
    "\n",
    "After you acquire the cluster and the client for the notebook, you would do a 'cluster.scale(jobs=1)' to actually request a worker pool that will be used for the execution of the subsequent cells.\n",
    "\n",
    "The recommendation is to cluster.scale(jobs=<up to 4>), do the compute, and then cluster.scale(jobs=0) to remove the cluster to preserve node hours.\n",
    "\n",
    "Currently, the underlying SLURMCluster object creates one slurm job which is limited to a 1 node allocation of a worker pool as per scale unit.  Scaling up to 4 jobs would mean 4 slurm jobs.  Note that 4 slurm jobs is the limit of the Andes cluster concurrently running.   If more nodes are needed, then each job would need to use job launchers such as 'srun', but unfortunately the current SLURMCluster is not compatible.\n",
    "\n",
    "In general, assume the notebook can run sequentially from top to bottom and be sure to scale up and scale down explicitely when possible.\n",
    "\n",
    "If cleanup is not done explicitely, the scheduler will be killed as the notebook's kernel is killed and then there the default 1 hour walltime defined underneath for the on-demand workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "labeled-grain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard address for the dask-labextension\n",
      "/proxy/59925\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.43.202.85:46665</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.43.202.85:59925/status' target='_blank'>http://10.43.202.85:59925/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.43.202.85:46665' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard preamble to use the Slurm cluster\n",
    "import random\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "# Slurm cluster submission to the Andes cluster\n",
    "# The cluster configuration is in ./etc/dask/dask.yml with sensible defaults\n",
    "# Refer to the \"dask.jobqueue.slurm\"\n",
    "dashboard_port = random.randint(10000,60000)\n",
    "cluster = SLURMCluster(\n",
    "    scheduler_options={\"dashboard_address\": f\":{dashboard_port}\"}\n",
    ")\n",
    "# We print out the address you copy into the dask-labextension\n",
    "print(\"Dashboard address for the dask-labextension\")\n",
    "print(f\"/proxy/{dashboard_port}\")\n",
    "\n",
    "# Create the client object\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-review",
   "metadata": {},
   "source": [
    "# Computation using the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-madison",
   "metadata": {},
   "source": [
    "## Initial data load, repartition, indexing & persist\n",
    "\n",
    "Below is an example of an ephemeral pre-loading step for your \"large\" dataset to be used in subsequent analysis.  You would need to load only the partitions necessary and would need to repartition to an adequate partition size (i.e., 100MB) and would also need to set_index to be able to acquire the divisions of the partitions.\n",
    "\n",
    "Once you do this, it would be wise to do a 'df = client.persist(df)' in the cluster and use the persisted dask dataframe in the subsequent cells.\n",
    "\n",
    "Here, you don't need to worry about the total DRAM your Dask cluster has.  The gears setup for the Andes cluster uses GPFS (/gpfs/alpine/scratch/<your_id>/.gears/dask/dask-worker-space) for as spill space, and the data you persist will be temporarily stored there.\n",
    "\n",
    "In the dask-scheduler dashboard (via dask-labextension), you would see 'orange' colored bars in the \"Dask Nbytes\" dashboard screen when this spill happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "living-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 952 ms, sys: 93.7 ms, total: 1.05 s\n",
      "Wall time: 1.04 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_power</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1347</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01 00:00:00</th>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01 00:31:00</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 23:28:43</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 23:59:59</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: read-parquet, 1347 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                    total_power\n",
       "npartitions=1347               \n",
       "2020-04-01 00:00:00     float32\n",
       "2020-04-01 00:31:00         ...\n",
       "...                         ...\n",
       "2020-04-29 23:28:43         ...\n",
       "2020-04-29 23:59:59         ...\n",
       "Dask Name: read-parquet, 1347 tasks"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import pandas as pd\n",
    "import fastparquet as fp\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Scale up right before running compute\n",
    "# Currently, 4 jobs is all you can do in an Andes cluster\n",
    "\n",
    "\n",
    "# Ensure temporary scratch space for this example\n",
    "import os\n",
    "SCRATCH = f\"{os.environ['MEMBERWORK']}/gen150/.gears/gears/examples\"\n",
    "\n",
    "DATASET = '/gpfs/alpine/stf218/proj-shared/stf008stc/openbmc.summit.raw/openbmc-202004*-*.parquet'\n",
    "PRECOMPUTE = f\"{SCRATCH}/total_power.parquet\"\n",
    "os.makedirs(SCRATCH, exist_ok=True)\n",
    "\n",
    "# Data preparation\n",
    "df = None\n",
    "if not os.access(PRECOMPUTE, os.F_OK):\n",
    "    # Load, repartition, set_index, only if we don't have it \n",
    "    cluster.scale(jobs=4)\n",
    "    df = dd.read_parquet(\n",
    "        DATASET, engine='fastparquet', index=False, gather_statistics=False,\n",
    "        columns=['timestamp', 'total_power'],\n",
    "    ).repartition(\n",
    "        partition_size=\"100MB\"\n",
    "    ).set_index(\n",
    "        'timestamp'\n",
    "    ).to_parquet(PRECOMPUTE, engine='fastparquet')\n",
    "    del df\n",
    "    cluster.scale(jobs=0)\n",
    "\n",
    "# Read parquet itself doesn't need a cluster\n",
    "df = dd.read_parquet(PRECOMPUTE, engine='fastparquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-evans",
   "metadata": {},
   "source": [
    "## Subsequent computation utilizing the persisted dataframe\n",
    "\n",
    "With the dataframe persisted in the ondemand cluster, you can now enjoy the optimized partition size, sorted index persisted in the ephemeral dask cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aerial-retail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 120 ms, total: 2.11 s\n",
      "Wall time: 8.26 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10849103948"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Look at the amount of records you're dealing with\n",
    "# This example uses 10 million records\n",
    "cluster.scale(jobs=2)\n",
    "value = df['total_power'].count().compute()\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "velvet-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.57 s, sys: 197 ms, total: 3.76 s\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267.99948"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Calculation utilizing the persisted dataset should be quicker\n",
    "cluster.scale(jobs=2)\n",
    "value = df['total_power'].std().compute()\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "attempted-certificate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.54 s, sys: 320 ms, total: 3.86 s\n",
      "Wall time: 15.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "646.25717447094"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cluster.scale(jobs=4)\n",
    "value = df['total_power'].mean().compute()\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the way how you debug the cluster\n",
    "cluster.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-mirror",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "Cleaning up the cluster\n",
    "Will be automatically curled up when the kernel dies but a good idea to explicitly do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "latin-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "pacific-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-storage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
