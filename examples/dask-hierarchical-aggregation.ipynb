{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "false-bibliography",
   "metadata": {},
   "source": [
    "# Hierarchical aggregation of temporal data at scale using Dask\n",
    "\n",
    "Hierarchical aggregation allows for efficient computation of mean, SD, min, and max aggregates at varying time resolutions, by resuing previous results and avoiding processing raw data as much as possible. The core idea is to store intermediate (\"lossless\") aggregates, which can be used to compute the final aggregates (mean, SD, min, and max) at current or coarser time resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-stick",
   "metadata": {},
   "source": [
    "## Code framework of hierarchical aggregation\n",
    "\n",
    "The framework contains utility tools for operating on dataframes (`rename`, `get_vars_from_hier_cols`, `get_vars_from_flat_cols`, and `get_cols`), and three main functions for producing the aggregates `init_agg`, `coarsen_agg`, and `finalize_agg`.\n",
    "\n",
    "### `init_agg` : raw → lossless\n",
    "`init_agg` is the only function to apply to raw temporal data. It produces intermediate (\"lossless\") aggregates at the requested time resolution. In an ideal use case, `init_agg` is called only once and for the finest time resolution possibly required. Its outputs are to be stored and subsequently used to compute the final aggregates, including for any coarser time resolution.\n",
    "\n",
    "### `coarsen_agg` : lossless → lossless\n",
    "`coarsen_agg` takes \"lossless\" aggregates and aggregates them further to some coarser time resolution. For precision, the latter should ideally be a multiple of the time resolution in the input data (e.g. coarsening aggregates at 5-second resolution into 1-minute resolution). For best performance, always choose the coarsest time resolution for the input (e.g. it's faster to obtain 1-minute aggregates from 30-second aggregates than from 5-second aggregates).\n",
    "\n",
    "### `finalize_agg` : lossless → final\n",
    "`finalize_agg` takes \"lossless\" aggregates and produces the final aggregates (mean, SD, min, and max) for the same time resolution. The final aggregates cannot be used for further coarsening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exempt-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def rename(df):\n",
    "    '''\n",
    "    Rename columns to allow element-wise operations between dataframes of the same size.\n",
    "\n",
    "    df: dataframe\n",
    "    '''\n",
    "    return df.rename(columns={col: i for i, col in enumerate(df.columns)})\n",
    "\n",
    "\n",
    "def get_vars_from_flat_cols(df, not_vars=[]):\n",
    "    '''\n",
    "    Extract variable names as the first part of flat column names.\n",
    "    \n",
    "    df: dataframe\n",
    "    not_vars: list of columns that should not be included as variables\n",
    "    '''\n",
    "    return pd.unique(['.'.join(col.split('.')[:-1]) for col in df.columns if col not in not_vars])\n",
    "    \n",
    "    \n",
    "def get_cols(variables, agg_type):\n",
    "    '''\n",
    "    Produce column names for a particular aggregate type.\n",
    "    \n",
    "    variables: iterable with variable names\n",
    "    agg_type: aggregate type identifier, e.g. 'count' or 'sum_sq_diff'\n",
    "    '''\n",
    "    return [f'{x}.{agg_type}' for x in variables]\n",
    "    \n",
    "\n",
    "def init_agg(df_raw, time_res, grouping_cols=[]):\n",
    "    '''\n",
    "    \"Losslessly\" aggregate raw data to given time resolution.\n",
    "    \n",
    "    df_raw: time-indexed dataframe to aggregate\n",
    "    time_res: time resolution (e.g. '15s' or '2min')\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # Extract names of variables to aggregate.\n",
    "    variables = [col for col in df_raw.columns if col not in grouping_cols]\n",
    "    \n",
    "    # Produce time column of the given time resolution.\n",
    "    time_col = df_raw.index.name\n",
    "    df_raw = df_raw.assign(**{time_col: lambda x: x.index.dt.floor(time_res).values})\n",
    "    df_raw[time_col] = dd.to_datetime(df_raw[time_col])\n",
    "    df_raw = df_raw.reset_index(drop=True)\n",
    "    \n",
    "    # Convert variable types to avoid overflow when computing variance.\n",
    "    df_raw[variables] = df_raw[variables].astype('float')\n",
    "    \n",
    "    # Produce intermediate aggregates at the given time resolution.\n",
    "    df_agg = df_raw.groupby(grouping_cols + [time_col]).agg(['count', 'sum', 'var', 'max', 'min'])\n",
    "\n",
    "    # Flatten column names of the intermediate aggregates.\n",
    "    cols = partial(get_cols, variables)\n",
    "    df_agg.columns = chain.from_iterable(\n",
    "        zip(cols('count'), cols('sum'), cols('sum_sq_diff'), cols('max'), cols('min')))\n",
    "\n",
    "    # Change variance to \"lossless\" sum of squared differences from mean.\n",
    "    sum_sq_diffs = rename(df_agg[cols('sum_sq_diff')])\n",
    "    counts = rename(df_agg[cols('count')])\n",
    "    df_agg[cols('sum_sq_diff')] = sum_sq_diffs.fillna(0) * (counts - 1)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def coarsen_agg(df_agg, time_res, grouping_cols=[]):\n",
    "    '''\n",
    "    Coarsen time resolution of \"losslessly\" aggregated data.\n",
    "    \n",
    "    df_agg: \"losslessly\" aggregated time- or multi-indexed dataframe\n",
    "    time_res: new time resolution (e.g. '15s' or '2min'), at least as coarse as the current one\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # Coarsen timestamp column to the given time resolution.\n",
    "    time_col = df_agg.index.name\n",
    "    df_coarse = df_agg.reset_index()\n",
    "    if not time_col: # index.name is None for multi-indices in Dask\n",
    "        time_col = {col for col in df_coarse if col not in set(df_agg.columns).union(grouping_cols)}.pop()\n",
    "    df_coarse[time_col] = df_coarse[time_col].dt.floor(time_res).values\n",
    "    \n",
    "    # Produce coarsened mean broadcasted to the original shape.\n",
    "    cols = partial(get_cols, get_vars_from_flat_cols(df_agg, not_vars=grouping_cols))\n",
    "    counts = rename(df_coarse[cols('count')])\n",
    "    sums = rename(df_coarse[cols('sum')])\n",
    "    df_coarse_grouped = df_coarse.groupby(grouping_cols + [time_col])\n",
    "    coarse_counts = rename(df_coarse_grouped[cols('count')].transform(\n",
    "        'sum', meta=df_coarse[cols('count')]._meta))\n",
    "    coarse_sums = rename(df_coarse_grouped[cols('sum')].transform(\n",
    "        'sum', meta=df_coarse[cols('sum')]._meta))\n",
    "    coarse_means = coarse_sums / coarse_counts\n",
    "    \n",
    "    # Produce sum of squared differences from coarsened mean.\n",
    "    sum_sq_diffs = rename(df_coarse[cols('sum_sq_diff')])\n",
    "    df_coarse[cols('sum_sq_diff')] = sum_sq_diffs + counts * (sums / counts - coarse_means)**2\n",
    "    \n",
    "    # Coarsen the \"lossless\" aggregates.\n",
    "    cols_to_agg = chain.from_iterable(\n",
    "        zip(cols('count'), cols('sum'), cols('sum_sq_diff'), cols('min'), cols('max')))\n",
    "    agg_functions = ['sum', 'sum', 'sum', 'min', 'max'] * len(cols(''))\n",
    "    df_coarse = df_coarse.groupby(grouping_cols + [time_col]).agg(dict(zip(cols_to_agg, agg_functions)))\n",
    "\n",
    "    return df_coarse\n",
    "\n",
    "\n",
    "def finalize_agg(df_agg, grouping_cols=[]):\n",
    "    '''\n",
    "    Compute final aggregates from losslessly aggregated data.\n",
    "    \n",
    "    df_agg: \"losslessly\" aggregated time- or multi-indexed dataframe\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # If time is the only index, replace it with the multi-index by using bogus aggregation.\n",
    "    time_col = df_agg.index.name\n",
    "    if time_col: # index.name is None for multi-indices in Dask\n",
    "        df_agg = df_agg.reset_index().groupby(grouping_cols + [time_col]).min()\n",
    "    \n",
    "    # Produce means and standard deviations.\n",
    "    cols = partial(get_cols, get_vars_from_flat_cols(df_agg))\n",
    "    counts = rename(df_agg[cols('count')])\n",
    "    sums = rename(df_agg[cols('sum')])\n",
    "    sum_sq_diffs = rename(df_agg[cols('sum_sq_diff')])\n",
    "    means = (sums / counts).rename(columns={old: new for old, new in zip(counts.columns, cols('mean'))})\n",
    "    stds = ((sum_sq_diffs / (counts - 1))**.5).rename(\n",
    "        columns={old: new for old, new in zip(counts.columns, cols('std'))})\n",
    "    \n",
    "    # Compose final aggregates and reoder columns by variable.\n",
    "    df_final = dd.concat([means, stds, df_agg[cols('min') + cols('max')]], axis=1, ignore_unknown_divisions=True)\n",
    "    df_final = df_final[list(chain.from_iterable(zip(cols('mean'), cols('std'), cols('min'), cols('max'))))]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-essence",
   "metadata": {},
   "source": [
    "## Setting up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard address for the dask-labextension\n",
      "/proxy/22941\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.43.202.87:46603</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.43.202.87:22941/status' target='_blank'>http://10.43.202.87:22941/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.43.202.87:46603' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "\n",
    "# Set up Slurm cluster.\n",
    "dashboard_port = random.randint(10000,60000)\n",
    "cluster = SLURMCluster(scheduler_options={\"dashboard_address\": f\":{dashboard_port}\"})\n",
    "\n",
    "# We print out the address you copy into the dask-labextension\n",
    "print(\"Dashboard address for the dask-labextension\")\n",
    "print(f\"/proxy/{dashboard_port}\")\n",
    "\n",
    "# Create the client object\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "iraqi-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-evanescence",
   "metadata": {},
   "source": [
    "## Reading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extreme-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.76 s, sys: 828 ms, total: 8.59 s\n",
      "Wall time: 43.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import fastparquet as fp\n",
    "\n",
    "# Ensure temporary scratch space for this example.\n",
    "TMP = f'{os.environ[\"MEMBERWORK\"]}/gen150/.gears/gears/examples'\n",
    "\n",
    "# Read raw data.\n",
    "RAW_DATA = '/gpfs/alpine/stf218/proj-shared/stf008stc/openbmc.summit.raw/openbmc-20200429-*.parquet'\n",
    "df_raw = dd.read_parquet(\n",
    "    RAW_DATA, engine='fastparquet', index='timestamp',\n",
    "    columns=['hostname', 'total_power', 'ambient'], gather_statistics=False, chunksize='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-allocation",
   "metadata": {},
   "source": [
    "## Hierarchical aggregation\n",
    "\n",
    "### Compute 15-second \"lossless\" aggregates from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "silent-suggestion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.7 ms, sys: 2.26 ms, total: 73 ms\n",
      "Wall time: 71.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_agg = init_agg(df_raw, '15s', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-ebony",
   "metadata": {},
   "source": [
    "### Store 15-second \"lossless\" aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "printable-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 1.07 s, total: 18.2 s\n",
      "Wall time: 4min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_15S_LOSSLESS = f'{TMP}/openbmc.summit.15s.lossless'\n",
    "os.makedirs(AGG_15S_LOSSLESS, exist_ok=True)\n",
    "\n",
    "# Reset index because Dask won't be able to recover the entire multi-index.\n",
    "df_agg.reset_index().to_parquet(AGG_15S_LOSSLESS, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-destination",
   "metadata": {},
   "source": [
    "### Compute 5-minute \"lossless\" aggregates from the multi-indexed output of `init_agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coordinate-wilson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 264 ms, sys: 2.37 ms, total: 266 ms\n",
      "Wall time: 264 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Compute 5-minute \"lossless\" aggregates from the 15-second ones.\n",
    "df_coarse = coarsen_agg(df_agg, '5min', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-summary",
   "metadata": {},
   "source": [
    "### ...or from its time-indexed disk version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "committed-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 7.81 ms, total: 200 ms\n",
      "Wall time: 298 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_agg = dd.read_parquet(\n",
    "    AGG_15S_LOSSLESS, engine='fastparquet', index='timestamp', gather_statistics=False, chunksize='100MB')\n",
    "df_coarse = coarsen_agg(df_agg, '5min', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-arrow",
   "metadata": {},
   "source": [
    "### Store 5-minute \"lossless\" aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 786 ms, sys: 59.4 ms, total: 845 ms\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_5MIN_LOSSLESS = f'{TMP}/openbmc.summit.15s.lossless'\n",
    "os.makedirs(AGG_5MIN_LOSSLESS, exist_ok=True)\n",
    "\n",
    "df_coarse.reset_index().to_parquet(AGG_5MIN_LOSSLESS, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-andrew",
   "metadata": {},
   "source": [
    "### Compute final aggregates at 5-minute resolution from the multi-indexed output of `coarse_agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "normal-johnston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 ms, sys: 0 ns, total: 29.4 ms\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_final = finalize_agg(df_coarse, grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-westminster",
   "metadata": {},
   "source": [
    "### ...or from its time-indexed disk version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thermal-bulletin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.6 ms, sys: 110 µs, total: 44.7 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_coarse = dd.read_parquet(\n",
    "    AGG_5MIN_LOSSLESS, engine='fastparquet', index='timestamp', gather_statistics=False, chunksize='100MB')\n",
    "df_final = finalize_agg(df_coarse, grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-pipeline",
   "metadata": {},
   "source": [
    "### Store 5-minute final aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sapphire-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 ms, sys: 5.84 ms, total: 109 ms\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_5MIN = f'{TMP}/openbmc.summit.5min'\n",
    "os.makedirs(AGG_5MIN, exist_ok=True)\n",
    "\n",
    "df_final.reset_index().to_parquet(AGG_5MIN, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-sandwich",
   "metadata": {},
   "source": [
    "### Check the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "understood-glucose",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>total_power.mean</th>\n",
       "      <th>total_power.std</th>\n",
       "      <th>total_power.min</th>\n",
       "      <th>total_power.max</th>\n",
       "      <th>ambient.mean</th>\n",
       "      <th>ambient.std</th>\n",
       "      <th>ambient.min</th>\n",
       "      <th>ambient.max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">a01n01</th>\n",
       "      <th>2020-04-29 00:00:00</th>\n",
       "      <td>1077.569811</td>\n",
       "      <td>540.773446</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2231.0</td>\n",
       "      <td>22.781132</td>\n",
       "      <td>0.414261</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 00:05:00</th>\n",
       "      <td>1018.244275</td>\n",
       "      <td>520.101104</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 00:10:00</th>\n",
       "      <td>853.309963</td>\n",
       "      <td>378.770956</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 00:15:00</th>\n",
       "      <td>780.363636</td>\n",
       "      <td>333.054690</td>\n",
       "      <td>568.0</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>22.325758</td>\n",
       "      <td>0.469547</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-29 00:20:00</th>\n",
       "      <td>671.046154</td>\n",
       "      <td>210.677923</td>\n",
       "      <td>557.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              total_power.mean  total_power.std  \\\n",
       "hostname timestamp                                                \n",
       "a01n01   2020-04-29 00:00:00       1077.569811       540.773446   \n",
       "         2020-04-29 00:05:00       1018.244275       520.101104   \n",
       "         2020-04-29 00:10:00        853.309963       378.770956   \n",
       "         2020-04-29 00:15:00        780.363636       333.054690   \n",
       "         2020-04-29 00:20:00        671.046154       210.677923   \n",
       "\n",
       "                              total_power.min  total_power.max  ambient.mean  \\\n",
       "hostname timestamp                                                             \n",
       "a01n01   2020-04-29 00:00:00            566.0           2231.0     22.781132   \n",
       "         2020-04-29 00:05:00            566.0           2229.0     23.000000   \n",
       "         2020-04-29 00:10:00            570.0           2135.0     23.000000   \n",
       "         2020-04-29 00:15:00            568.0           2136.0     22.325758   \n",
       "         2020-04-29 00:20:00            557.0           1521.0     22.000000   \n",
       "\n",
       "                              ambient.std  ambient.min  ambient.max  \n",
       "hostname timestamp                                                   \n",
       "a01n01   2020-04-29 00:00:00     0.414261         22.0         23.0  \n",
       "         2020-04-29 00:05:00     0.000000         23.0         23.0  \n",
       "         2020-04-29 00:10:00     0.000000         23.0         23.0  \n",
       "         2020-04-29 00:15:00     0.469547         22.0         23.0  \n",
       "         2020-04-29 00:20:00     0.000000         22.0         22.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-gospel",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "Cleaning up the cluster\n",
    "Will be automatically curled up when the kernel dies but a good idea to explicitly do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "binary-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "signal-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
